<!DOCTYPE HTML>
<html>

	<head>

		<title>Roberto A. Ibanez</title>

		<meta charset="utf-8" />

		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

		<link rel="stylesheet" href="assets/css/main.css" />

		<link rel="stylesheet" href="assets/css/ie9.css" />

		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<style>p{margin: 12px}</style>

	</head>

	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">

						<div class="logo">

							<span class="icon fa-user"></span>

						</div>

						<div class="content">

							<div class="inner">

								<h1>Roberto A. Ibañez</h1>

								<p><br>Un apasionado por el aprendizaje de máquinas.</p>

							</div>

						</div>

						<nav>

							<ul>

								<li><a href="#intro">Intro</a></li>

								<li><a href="#about">About</a></li>

								<li><a href="#work">Work</a></li>

								<li><a href="#cv">CV</a></li>

								<li><a href="#contact">Contact</a></li>

							</ul>

						</nav>

					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">

								<h2 class="major">About Me</h2>

								<div id="container">

									<div id="photo">

										<p> </p>

										<img src="images/recorte800x1000.png" alt="" width="140" />

									</div>

									<div id="content">

										<p style="font-size: 30px; font-family: Cambria, 'Hoefler Text', 'Liberation Serif', Times, 'Times New Roman', serif;"><strong>Roberto A. Ibanez </strong></p>

										<p> <a href="http://proterabio.com"> Ingeniero en Inteligencia Artificial en Protera Bio</a> </p>

										<p>	<a href="http://ingenieria.uchile.cl">Bachelor en Ciencias de La Ingeniería, Mención Mecánica</a> </p>

                                        <p>	<a href="http://uchile.cl">Universidad de Chile.</a> <br> <br> </p>

									</div>

								</div>

                                <div id="container2">

                                	<p>_</p>

									<p style="font-size: 25px"><strong>Intro:</strong></p>

									<p>No basta solo con cuestionarse dónde se vive, a qué se pertenece o cuál es nuestro origen. Hoy en día ese planteamiento como eje resolutivo de la vida está obsoleto si nos atrevemos a mirar los beneficios de la tecnología que nos circunda y, más aún, si queremos sacarle el mejor provecho y formar parte de ella.</p>

									<p>Somos partícipes de un mundo que se declara globalizado, sin embargo, el cuándo empezó a ocurrir aquello, qué significa y de qué manera nos impacta, son preguntas que al ser contestadas nos rebelará que el horizonte se ha movido, y por mucho, y que el miedo que le podríamos tener a los cambios está fuera de toda lógica, pues sería un miedo tardío y artificial.</p>

									<p>La tecnología como está instaurada y cómo se piensa a futuro nos promueve a la libertad y a reconocernos como colineales en el espacio de las posibilidades y oportunidades. Junto con ella tenemos el potencial de modificar la forma en que vivimos, de redefinir el cómo nos relacionamos, en resumen, de crear sociedad.</p>

									<p>Pertenezco a una generación que se inició en un Chile en aras de mayor integración, que ha tenido resilencia de unir un tiempo de precario acceso a la tecnología y uno en que el acceso es inmediato. He visto ambos mundos y culturas, es más, cuando me detengo a observar las generaciones que me anteceden y preceden llego a la convicción que todos podemos convivir bajo una tecnología de bienestar, que nos sirva para tomar decisiones menos sesgadas y mejores, conservando valores humanos, nuestra naturaleza; ser autovalentes en lo que decidamos generar y así conservar nuestra creatividad.</p>

									<p>Criado bajo el alero de la educación pública, de principio me llamó la atención el desarrollo de la tecnología. Decidí estudiar ingeniería, centrando mi interés en la mecánica y la computación.</p>

									<p>Si durante la revolución industrial el trabajo físico que producía el hombre fue mecanizado por complejos sistemas mecánicos, hoy es cuando el trabajo cognitivo ha comenzado a ser mecanizado. Las máquinas aprenden, se humanizan, son un aleado. No solo ejecutan trabajo mecánico sino que realizan tareas complejas, poseen habilidades cognitivas. Tenemos la posibilidad de forjar una nueva era, la era de las máquinas inteligentes.</p>

                                    <p>Santiago, 2018</p>

								</div>

							</article>

							<article id = "pub">

								<h2 class="major">Academics</h2>

								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>

								<p style="font-size: 25px"><strong>Publications:</strong></p>

								<p>

									Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification<br>
									<strong>Junyang Lin</strong>, Qi Su, Pengcheng Yang, Shuming Ma and Xu Sun<br>
									To appear in EMNLP 2018<br><br>
									Learning When to Concentrate or Divert Attention: Automatic Control of Attention Temperature for Neural Machine Translation<br>
									<strong>Junyang Lin</strong>, Xu Sun, Xuancheng Ren, Muyu Li and Qi Su<br>
									To appear in EMNLP 2018<br><br>
									DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text [<a href="https://arxiv.org/abs/1802.01345">pdf</a>]<br>
                                    Jingjing Xu, Xuancheng Ren, <strong>Junyang Lin</strong> and Xu Sun<br>
                                    To appear in EMNLP 2018<br><br>
                                    An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation<br>
                                    Jingjing Xu, Liangchen Luo, <strong>Junyang Lin</strong>, Qi Zeng and Xu Sun<br>
                                    To appear in EMNLP 2018<br><br>
									Deconvolution-Based Global Decoding for Neural Machine Translation<br>
									<strong>Junyang Lin</strong>, Xu Sun, Xuancheng Ren, Shuming Ma, Jinsong Su and Qi Su<br>
									To appear in COLING 2018<br><br>
									Global Encoding for Abstractive Summarization [<a href="https://arxiv.org/abs/1805.03989">pdf</a>]<br>
                                    <strong>Junyang Lin</strong>, Xu Sun, Shuming Ma and Qi Su<br>
                                    ACL 2018<br><br>
                                    Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization<br>
                                    Shuming Ma, Xu Sun, <strong>Junyang Lin</strong> and Houfeng Wang<br>
                                    ACL 2018<br><br>
                                    Bag-of-Words as Target for Neural Machine Translation<br>
                                    Shuming Ma, Xu Sun, Yizhong Wang and <strong>Junyang Lin</strong><br>
                                    ACL 2018<br><br>
                                    A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification [<a href="https://arxiv.org/abs/1805.01089">pdf</a>]<br>
                                    Shuming Ma, Xu Sun, <strong>Junyang Lin</strong> and Xuancheng Ren<br>
                                    IJCAI 2018<br><br>
                                    Decoding-History-Based Adaptive Control of Attention for Neural Machine Translation [<a href="https://arxiv.org/abs/1802.01812">pdf</a>]<br>
                                    <strong>Junyang Lin</strong>, Shuming Ma, Qi Su and Xu Sun<br>
                                    arXiv<br><br>
								</p>
								<p style="font-size: 25px"><strong>Technical Reports:</strong></p>
								<p>
									Learn the Dependencies Between Content Words and Function Words [<a href="http://shumingma.com/learn-dependencies-content.pdf">pdf</a>]<br>
									<strong>Junyang Lin</strong><br><br><br>
								</p>
								<p style="font-size: 25px"><strong>Academic Activities:</strong></p>
								<p>
									Reviewer for ACL 2018, EMNLP 2018, IJCAI 2018, NAACL 2018, EMNLP 2017, CLSW 2017.<br>
								</p>
							</article>

						<!-- Work -->
							<article id="blog">
								<h2 class="major">Blog</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p><a href="#DLPPT">Introduction to Deep Learning (PPT)</a>.</p>
								<p><a href="#SVM2">Another Perspective to View SVM (2)</a>.</p>
								<p><a href="#SVM">Another Perspective to View SVM (1)</a>.</p>
								
							</article>

							<article id="SVM">
								<h3 class="major">Another Perspective to View SVM (1)</h3>
								<!-- <span class="image main"><img src="images/pic02.jpg" alt="" /></span> -->
								<p>This is what I can never forget. Nine months ago, my mentor said to me:"Hey, how about building a paraphrasing system for your final project? Well try to do some feature engineering, label your data, and use SVM?" Ooh, maan, I can only write no more than ten lines of code at that time and you told me to build a system with a Support Vector Machine?! Is it a machine that I can afford? Er mar gerd... Back to our topic, since then, I have had a special feeling for SVM and I have always been curious about such black magic. In the following content, I am going to focus on the loss function of SVM and it involves some knowledge about square error and cross entropy.</p>
								<p>Support Vector Machine has nothing to do with a real machine. It is just a great machine learning algorithm which often has excellent performance in binary classification. Training a SVM model helps us find the best hyperplane (just like a plane in the high dimensional space) to separate the data and classify them to the right categories. Its main difference from perceptron is that it not only separates the data but also maximizes the margin. Here I do not want to discuss about details about functional margin and geometric margin. I try to illustrate it in a more intuitive way. Say, you have a bunch of data, like ten thousand documents with five thousand of positive attitude and another five thousand of negative attitude, and you want to train a model that can classify them. The procedure is relatively simple: you just need to turn the documents into feature vectors with labels, $+1$ for positive and $-1$ for negative. Suppose the data are linear separable, there should be a hyperplane that can best separate them with largest margin. Intuitively, on either side, there should be some vectors that are nearest to the hyperplane, which are support vectors. These vectors are on two hyperplanes that are parallel to the best hyperplane, $H_1$ and $H_2$, and the distance between $H_1$ and $H_2$ is margin. This is not difficult, but the problem is, how can we find such hyperplane? If you find some books to read, you will see a lot about convex quadratic programming using Lagrange function, which is really puzzling. However, I learned a lot from Hung-yi Lee, a professor in NTU, that training SVM can be viewed from another perspective, which I think is much simpler. If you know something about Pegasos, the fact that SVM can be trained with gradient descent may seem obvious to you. In the following parts, I am going to discuss about this issue.</p>
								<p> </p>
								<p>Let's define the functions for linear separable SVM first. We have a bunch of feature vectors \(\mathcal{X} = \{x^{(1)}, x^{(2)}, ..., x^{(n)}\}\), corresponding to the label space \(\mathcal{Y} = \{+1, -1\}\), and now we use an affine function \(f(x) = w^{T}x + b\) to model the data points, and our task is to find the best hyperplane \(w^{*T}x + b^{*} = 0\) to separate them. For a certain data point $x^{(i)}$, if it is correctly classified, \(y^{(i)}f(x^{(i)}) > 0\). This is simple, but adding margin maximization problem to it makes it much more complicated. Instead of discussing about the functional and geometrical margin as well as convex quadratic programming, I would like to talk about the choice of loss function for a linear classifier.</p>
								<p>(From Hung-yi Lee's PPT in the Lecture 20 of Machine Learning)</p>
								<p>As is shown in the picture, the x axis represents $y^{(n)}f(x^{(n)})$, while the y axis represents the loss. In fact we have an ideal loss, which is \(l(g(x^{(n)}), y^{(n)}) = \delta(g(x^{(n)}) \neq y^{(n)})\), where \(g(x^{(n)}) = sign(f(x^{(n)}))\). However, it is problematic in application since it is totally not differentiable. Therefore, we need to find some alternatives, including L2 loss, sigmoid square loss, cross entropy loss and hinge loss. First of all, it is impossible to train a linear classifier with L2 loss function (\(l(f(x^{(n)}), y^{(n)}) = (y^{(n)}f(x^{(n)})-1)^2\), see the red curve). We hope that the target $y^{(n)}$ and the output $f(x^{(n)})$ are of the same sign, and we also hope that the larger their product is, the smaller the loss is, so using mean square error here is definitely inapproapriate. Second of all, sigmoid square loss is also not a good choice (\(l(f(x^{(n)}), y^{(n)}) = (\sigma(y^{(n)}f(x^{(n)}))-1)^2\)). If you have ever experienced the problem of vanishing gradient in your training of neural network, you can find that the problem here is similar. If the value $y^{(n)}f(x^{(n)})$ is a large negative number, its derivative is close to $0$, which means the gradient is too small for the loss to move downward. So how about cross entropy loss? I think it is a reasonable choice and its performance may be satisfactory. The loss function is \(l(f(x^{(n)}), y^{(n)}) = log(1 + exp(-y^{(n)}f(x^{(n)}))\). It seems great since it is the upper bound of the ideal loss and it is consistent with the rule "the larger value, the smaller loss". Moreover, when the $y^{(n)}f(x^{(n)})$ is negative, it has large gradient, which encourages the loss to move downward. However, is there even a better loss function? We may find that the problem of this function is that it encourages the value to be as large as possible, but in fact, if the value is larger than a certain number, our requirements are all satisfied and there is no need for it to be even larger. It may be not robust enough and may be influenced by the outliers.</p>
								<p>(From Andrew Zisserman's PPT in the Lecture 2 of Machine Learning)</p>
								<p>This function is pretty simple: $$l(f(x^{(n)}), y^{(n)}) = max(0, 1 - y^{(n)}f(x^{(n)}))$$ It means that when the value is negative, the loss is large and it encourages the value to become positive. Moreover, being positive is not enough and it should be larger than a certain value, here we set the value $1$. When the value is larger than 1, there is no need to decrease the loss because we now have enough confidence that we have correctly classified the data points, which solves the problem mentioned above. The value $1$ is actually our margin. You can try to prove it from the perspective of functional margin and geometrical margin. Now there is a problem about the value of the margin. The reason why it should be $1$ is that it makes the loss function a tight, or in another word, lowest upper bound of the ideal loss.</p>
								<p>Now that the problem becomes clear, we can build the final loss function for linear SVM: $$L(f) = \sum_{i=1}^{n}l(f(x^{(i)}), y^{(i)}) + \frac{\lambda}{2}||w||_2^2$$ Well, is it similar to the optimization problem that you are familiar with? $$min\frac{1}{2}||w||_2^2 + C\sum_{i=1}^{n}\xi^{(i)}$$ $$subject \; to \; y^{(i)}f(x^{(i)}) \geq 1 - \xi^{(i)}$$ Here the $\xi^{(i)}$ is our slack variable, which is also our hinge loss function. Wow, it is just like magic!</p>
								<p>I have to admit that the first time I learned these ideas I jumped off my chair excitedly. Even today I still feel that this perspective is really great for beginners to understand linear SVM. I hope I can share the happiness to you guys. Next time I will try to discuss about kernel tricks in SVM, starting from gradient descent of SVM.</p>
							</article>

							<article id="SVM2">
								<h3 class="major">Another Perspective to View SVM (2)</h3>
								<p>In this part, I am going to prove that $w$ is the linear combination of the inputs $x$. In the last passage, I discussed about the loss function of SVM and showed that it can be trained with gradient descent. Here I would like to present the training steps by calculating the gradients.</p>
								<p>The hinge loss function of SVM is: $$l(f(x^{(n)}), \hat{y}^{(n)})=max(0, 1 - \hat{y}^{(n)}f(x^{(n)}))$$ The derivative of $l(f(x^{(n)}), \hat{y}^{(n)})$ to $w_i$ can be calculated below: $$\frac{\partial{l}}{\partial{w_i}}=\frac{\partial{l}}{\partial{f}}\frac{\partial{f}}{\partial{w_i}}=\begin{cases} -\hat{y}^{(n)}x^{(n)}_i& 1-\hat{y}^{(n)}f(x^{(n)}) > 0 \\ 0& 1-\hat{y}^{(n)}f(x^{(n)}) \le 0 \end{cases}$$ so the gradient of $w_i$ is: $$\nabla{w_i}=\frac{\partial{L}}{\partial{w_i}}=\sum_{n}-\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}_{i}$$ and the gradient of the weight vector is: $$\nabla{w}=\frac{\partial{L}}{\partial{w}}=\sum_{n}-\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}$$ In the optimization procedure, the weight vector $w$ is optimized in each step in the following way: $$w=w-\eta\nabla{w}$$ If the weight vector reaches its global minimum in $m$ steps, which means the optimal $w^{*}$ is obtained, \nabla{w}=0. Therefore, if $w$ is initialized as $0$, $w^{*}$ can be presented as follows: $$w^{*}=m\cdot(-\eta\nabla{w})=\sum_{n}m\eta\delta(\hat{y}^{(n)}f(x^{(n)})<1)\hat{y}^{(n)}x^{(n)}$$ Now I define a term called $\alpha_{n}$, that: $$\alpha_{n}=m\eta\delta(\hat{y}^{(n)}f(x^{(n)})<1)$$ Then $w^{*}$ can be written as below: $$w^{*}=\sum_{n}\alpha_{n}\hat{y}^{(n)}x^{(n)}$$ This equation is the same of $w^{*}$ in the Lagrange function for solving the dual problem of SVM, and it proves that $w$ is a linear combination of the data points $x$. Here the equation can be further simplified to: $$w=\sum_{n}\alpha_{n}^{*}x^{(n)}=X\alpha^{*}$$ It should be noted that because of $\delta$ in $\alpha^{*}$, the vector $\alpha^{*}$ is sparse. The vectors $x^{(n)}$ with non-zero $\alpha_{n}^{*}$ are the support vectors holding the margin.</p>
								<p>Back to the affine function $f(x)=w^{T}x$, now it can be changed to: $$f(x)=\alpha^{T}X^{T}x=\sum_{n}\alpha_{n}(x_{n}\cdot{x})$$ Here I define a function called K that: $$K(x_{n}, x)=\sum_{n}x_{n}\cdot{x}$$ so: $$f(x)=\sum_{n}\alpha_{n}K(x_{n}\cdot{x})$$ Function K is called kernel function (here this K function is linear kernel function), which projects the input data from the input space to the feature space, usually of higher dimensions. The calculation of $K(x, z)$ is much simpler than projection function $\phi(x)$. Moreover, the power of kernel function is more than simple calculation. It is able to project the inputs from the inputs space to the space of infinite dimensions. One typical example is the well-known RBF function.</p>
								<p>In the next passage, I am going to introduce the traditional method to solve the primal problem of SVM, and try my best to interpret Lagrange multiplier, which has puzzled me for a long time, compared with L2 regularization.</p>
							</article>

							<article id="DLPPT">
								<h3 class="major">Intro to Deep Learning (PPT)</h3>
								<p>The following link is my PPT for a lecture in the course Computational Linguistics at our institute, which is a simple introduction to deep learning. If you are interested, <a href="DEEP LEARNING.pdf">please click here!</a></p>
							</article>

						<!-- About -->
							<article id="cv">
								<h2 class="major">CV</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<p>To read my curriculum vitae, <a href="ribanez_resume_es.pdf">please click here!</a></p>
							</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
<!-- 								<form method="post" action="#">
									<div class="field half first">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field half">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="4"></textarea>
									</div>
									<ul class="actions">
										<li><input type="submit" value="Send Message" class="special" /></li>
										<li><input type="reset" value="Reset" /></li>
									</ul>
								</form> -->
								<ul class="icons">
									<li><a href="mailto:roberto.ibanez@ug.uchile.cl" class="icon fa-envelope-square"><span class="label">Email</span></a></li>
									<!-- <li><a href="https://www.linkedin.com/in/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li> -->
									<li><a href="https://github.com/ribanez" class="icon fa-github"><span class="label">GitHub</span></a></li>
								</ul>
								<p>Email: roberto.ibanez@ug.uchile.cl</p>
							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; ribanez </p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
